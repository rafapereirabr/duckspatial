---
title: "Benchmark"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
code-annotations: hover
urlcolor: blue
vignette: >
  %\VignetteIndexEntry{Benchmark}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
  
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = identical(tolower(Sys.getenv("NOT_CRAN")), "true"),
  out.width = "100%"
)

# Initialize variables to prevent "object not found" error when eval=FALSE
time_diff <- "XX"
memo_diff <- "XX"

# CRAN OMP THREAD LIMIT to avoid CRAN NOTE
Sys.setenv(OMP_THREAD_LIMIT = 2)
```

This vignette shows a few benchmarks comparing **{duckspatial}** vs **{sf}**. We look 
at how both packages compare in terms of computation time and memory use when 
performing different spatial operations with increasingly large data sets. We plan 
to  extend this vignette in the future to benchmark other types of spatial operations.

# Spatial Join

Here we analyze how {duckspatial} and {sf} compare  when performing a spatial join 
between points and polygons with increasingly large numbers of points.

## TL;DR

- {sf} is faster for small data sets, when the time and memory differences really 
don't matter that much. However, for large data sets (e.g. above 100K points), 
{duckspatial} is much faster and uses way less memory. 

```{r, message = FALSE}
library(duckspatial)
library(bench)
library(dplyr)
library(ggplot2)
options(scipen = 999)

# read polygons data
countries_sf <- sf::st_read(system.file("spatial/countries.geojson", package = "duckspatial"))

run_benchmark <- function(n){
    
    set.seed(42)

    ## create points data
    points_sf <- data.frame(
        id = 1:n,
        x = runif(n, min = -180, max = 180),  
        y = runif(n, min = -90, max = 90)
        ) |> 
        sf::st_as_sf(coords = c("x", "y"), crs = 4326)
    
    temp_bench <- bench::mark(
        iterations = 1, 
        check = FALSE, 
        duckspatial = duckspatial::ddbs_join(
            x = points_sf, 
            y = countries_sf, 
            join = "within"),
        
        sf = sf::st_join(
            x = points_sf, 
            y = countries_sf, 
            join = sf::st_within)
        )
    
    temp_bench$n <- n
    temp_bench$pkg <- c("duckspatial", "sf")
    
    return(temp_bench)
}


# From 100K, to 1 million and 10 million points
df_bench <- lapply(
    X = c(10e3, 10e4, 10e6),
    FUN = run_benchmark
    ) |> 
    dplyr::bind_rows()

```

```{r, echo=FALSE}
# calculate difference in performance
temp <- df_bench |> 
    filter(n == 10e6)

memo_diff <- round(as.numeric(temp$mem_alloc[2] / temp$mem_alloc[1]),1)
time_diff <- (1 - round(as.numeric(temp$median[1] / temp$median[2]),1))*100
```

Now let's have a look at the results.


As one would expect, {sf} is faster for small data sets, when the time 
difference is less than a couple seconds. For larger data sets, though, 
{duckspatial} gets much more efficient. In this example working with 10 million 
points, {duckspatial} was `r time_diff`% faster and used `r memo_diff` times less 
memory than {sf}. Not bad.


```{r, warning=FALSE}
ggplot(data = df_bench) +
    geom_point(size =3, aes(x= mem_alloc, y = median, color = pkg, 
                    shape = format(n, big.mark = ".")
                    )) +
    labs(color= "Package", shape = "Data size",
         y = "Computation time (seconds)",
         x = "Memory allocated") +
    theme_minimal()


```
